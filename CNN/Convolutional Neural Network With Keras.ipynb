{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Classification Using CNN\n",
    "\n",
    "#### Convolutional Neural Network is widely used in Computer Vision Task. Its been applied in Self Driving Cars and Face recoginition.\n",
    "\n",
    "** CNN is an extended form of Artifical Neural Network i.e. before ANN we perform a convolution followed by pooling and a full connection i.e ANN. **\n",
    "\n",
    "**Convolution Flow:** \n",
    "\n",
    "    ðŸ˜Š   ==> CNN ==> HAPPY \n",
    "    \n",
    "   **input_image ==> Convolution ==> RELU ==> Pooling ==> Flattening ==> Full Connection**\n",
    "\n",
    "   **input_image ==> CNN ==> output_class**\n",
    "\n",
    "       \n",
    "        \n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution :\n",
    "\n",
    "** A image is a combination of pixels i.e. (Width x Height) spread across the dimension as (N x M) dimension. If I have 32 * 32 dimension image, it means i have 32 rows and each row has 32 columns with each column containing value between (0-255).**\n",
    "\n",
    "** Now not all values present in all columns are valuable and mostly it contains noise in the background. So our aim is to get the best out of the whole image and we will achieve it by Feature Detector.**\n",
    "\n",
    "** Feature Detector is a subsample of 32 x 32, we can have n # of Feature Detector from an Image of 32 x 32 with dimension less than (N x M) here it is less than (32 x 32) and apply it over the image and extract the Feature Map so based on Feature Detector, we have different Different Maps containing various feature of the same Image.**\n",
    "\n",
    "** What is Feature Map ? A Feature Map is again reduced form of orginial image with most of the feature kept intact in it.**\n",
    "\n",
    "\n",
    "** In short, Convoultion is to find the \"Features\" using \"Feature Detector\" and putting it into \"Feature Map\" **\n",
    "\n",
    "** RELU : Why RELU ? When we apply feature detector we tend to make the image Linear i.e. it progress like Black to slowly Gray then to White like Linear progression. But we know images are never linear they are highly non linear i.e. the contrast of the image with up and down not in linear fashion, So to break the linearity we use RELU Function **\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling :\n",
    "\n",
    "** It maintains the Spatial Invariance. Now after we have captured the feature in Feature Map, we need something called as pooling i.e. i have 10 images of DOG, Not every dog will be in the same position in the image, one might be walking other might be sitting but the Feature will be same for the DOG. Features are independent of the relatives of the object present in the image. ** \n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening :\n",
    "\n",
    "** Flattening helps us in turn our Matrix into a input to an ANN(Artifical Neural Network.**\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Connection :\n",
    "\n",
    "** A Full connection refers to all neurons present is Input Layer, hidden Layer and Output Layer are fully connected i.e. all the input from input layer is connected to all the Hidden Layers and all the Hidden layers are connected to all the Output Layers** \n",
    "\n",
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note :\n",
    "\n",
    "** While performing pooling, we lose around 75% less important feature **\n",
    "** We also reduce overfitting by removing insigificant feature **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayurjain/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Convolution2D(32,3,3, input_shape = (64,64,3),activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** In Sequential class, we are adding a convolution of 32 Feature Detector with 3 x 3 dimension while we are taking **\n",
    "\n",
    "**input in 64 x 64 dimension with 3 refers to RGB and performing RELU activation and the end to introduce the non linearity to the image or feature map. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Pooling operation, We condense the feature map using the pooling operation by getting only the significant feature.**\n",
    "\n",
    "** We are performing the same operation indepth of the image in the below steps before moving to flattening**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayurjain/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** A ANN is performed by connecting our flattened output as input to the ANN and performing usual ANN operation such as optimization and measusing cross entropy loss.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayurjain/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", units=128)`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/mayurjain/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"sigmoid\", units=1)`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(output_dim = 128,kernel_initializer='uniform' ,activation = 'relu'))\n",
    "classifier.add(Dense(output_dim = 1,kernel_initializer='uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayurjain/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \"\"\"\n",
      "/Users/mayurjain/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=<keras.pre..., steps_per_epoch=250, epochs=25, validation_steps=2000)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 733s 3s/step - loss: 0.6655 - acc: 0.5929 - val_loss: 0.6170 - val_acc: 0.6669\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 670s 3s/step - loss: 0.5934 - acc: 0.6791 - val_loss: 0.5702 - val_acc: 0.7061\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 685s 3s/step - loss: 0.5635 - acc: 0.7069 - val_loss: 0.5414 - val_acc: 0.7244\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 692s 3s/step - loss: 0.5290 - acc: 0.7361 - val_loss: 0.5021 - val_acc: 0.7609\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 687s 3s/step - loss: 0.5113 - acc: 0.7411 - val_loss: 0.5134 - val_acc: 0.7564\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 671s 3s/step - loss: 0.4912 - acc: 0.7632 - val_loss: 0.4823 - val_acc: 0.7734\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 681s 3s/step - loss: 0.4768 - acc: 0.7705 - val_loss: 0.5014 - val_acc: 0.7651\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 679s 3s/step - loss: 0.4627 - acc: 0.7812 - val_loss: 0.4615 - val_acc: 0.7870\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 689s 3s/step - loss: 0.4534 - acc: 0.7835 - val_loss: 0.4639 - val_acc: 0.7766\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 670s 3s/step - loss: 0.4397 - acc: 0.7935 - val_loss: 0.4619 - val_acc: 0.7864\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 680s 3s/step - loss: 0.4301 - acc: 0.8044 - val_loss: 0.4597 - val_acc: 0.7917\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 691s 3s/step - loss: 0.4165 - acc: 0.8101 - val_loss: 0.4631 - val_acc: 0.7871\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 684s 3s/step - loss: 0.4059 - acc: 0.8137 - val_loss: 0.4571 - val_acc: 0.7826\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 707s 3s/step - loss: 0.3970 - acc: 0.8189 - val_loss: 0.4689 - val_acc: 0.7915\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 697s 3s/step - loss: 0.3843 - acc: 0.8265 - val_loss: 0.4397 - val_acc: 0.8037\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 711s 3s/step - loss: 0.3752 - acc: 0.8253 - val_loss: 0.4267 - val_acc: 0.8105\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 687s 3s/step - loss: 0.3559 - acc: 0.8416 - val_loss: 0.4420 - val_acc: 0.8096\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 680s 3s/step - loss: 0.3450 - acc: 0.8504 - val_loss: 0.4422 - val_acc: 0.8050\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 704s 3s/step - loss: 0.3354 - acc: 0.8514 - val_loss: 0.4471 - val_acc: 0.8024\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 728s 3s/step - loss: 0.3284 - acc: 0.8546 - val_loss: 0.4478 - val_acc: 0.8049\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 722s 3s/step - loss: 0.3153 - acc: 0.8603 - val_loss: 0.4924 - val_acc: 0.7906\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 699s 3s/step - loss: 0.3029 - acc: 0.8647 - val_loss: 0.4387 - val_acc: 0.8120\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 704s 3s/step - loss: 0.2974 - acc: 0.8759 - val_loss: 0.4454 - val_acc: 0.8137\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 675s 3s/step - loss: 0.2788 - acc: 0.8786 - val_loss: 0.4648 - val_acc: 0.8200\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 679s 3s/step - loss: 0.2734 - acc: 0.8809 - val_loss: 0.5496 - val_acc: 0.7913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1129597b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(training_set,\n",
    "                         samples_per_epoch = 8000,\n",
    "                         nb_epoch = 25,\n",
    "                         validation_data = test_set,\n",
    "                         nb_val_samples = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_to_json = classifier.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Folder_structure/classifier.json\", \"w\") as json_file:\n",
    "    json_file.write(classifier_to_json)\n",
    "# serialize weights to HDF5\n",
    "classifier.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open('/Folder_structure/classifier.json','r')\n",
    "loaded_classifier_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_classifier_json)\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_img = image.load_img('/Folder_structure/dog_or_cat_1.jpeg',target_size=(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_img = image.img_to_array(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_img = np.expand_dims(test_img,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Note :\n",
    "\n",
    "** The following blog are very helpful in understanding CNN **\n",
    "\n",
    "http://cs231n.github.io/convolutional-networks/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
